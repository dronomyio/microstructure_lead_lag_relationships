```
// Aligned memory allocation for SIMD
template<typename T>
T* aligned_alloc(size_t n, size_t alignment = 64) {
    void* ptr = nullptr; 
    if (posix_memalign(&ptr, alignment, n * sizeof(T)) != 0) 
    { 
      throw std::bad_alloc(); 
    } 
    return static_cast<T*>(ptr);
}
```
Let me visualize exactly how `posix_memalign` works step by step:

## **posix_memalign() - Visual Step-by-Step**

### **1. Initial State - Before posix_memalign**

```
STACK (Your Program):
┌────────────────────────┐
│  void* ptr = nullptr;  │
│  ┌──────────┐          │
│  │   ptr    │────→ NULL│
│  └──────────┘          │
└────────────────────────┘

HEAP (System Memory):
┌──────────────────────────────────────────────────────────┐
│ ???  Random unallocated memory with random addresses  ???│
└──────────────────────────────────────────────────────────┘
```

### **2. Calling posix_memalign(&ptr, alignment, n * sizeof(T))**

```cpp
posix_memalign(&ptr, 64, 16 * sizeof(float))
               ↑     ↑    ↑
               │     │    └── Size: 16 floats × 4 bytes = 64 bytes
               │     └────── Alignment: 64-byte boundary
               └──────────── Pass ADDRESS of ptr (not ptr itself!)
```

**What happens internally:**

```
Step 2a: System searches for suitable memory
─────────────────────────────────────────────

HEAP Memory Map:
Address:    0x1000  0x1013  0x1037  0x1040  0x1055  0x1080  0x10C0
            ↓       ↓       ↓       ↓       ↓       ↓       ↓
┌───────────┬───────┬───────┬───────┬───────┬───────┬───────┬────
│  Used     │ Free  │ Used  │ Free  │ Used  │ Free  │ Free  │...
└───────────┴───────┴───────┴───────┴───────┴───────┴───────┴────
            ↑               ↑               ↑       ↑
            19 bytes        9 bytes         43 bytes│
            (TOO SMALL)     (TOO SMALL)     (OK but not aligned!)
                                                     ↑
                                            0x1080 is aligned to 64!
                                            (0x1080 = 4224 = 64 × 66)

Step 2b: System finds aligned address
──────────────────────────────────────

Alignment Check:
0x1040 ÷ 64 = 65.0    → ✓ Perfectly divisible! (0x1040 = 64 × 65)
0x1055 ÷ 64 = 65.328  → ✗ Not aligned
0x1080 ÷ 64 = 66.0    → ✓ Perfectly divisible! (0x1080 = 64 × 66)
0x10C0 ÷ 64 = 67.0    → ✓ Perfectly divisible! (0x10C0 = 64 × 67)

System picks 0x1080 (first available aligned address with enough space)
```

### **3. After Successful Allocation**

```cpp
if (posix_memalign(&ptr, 64, 64) != 0)  // Returns 0 on success
```

```
STACK (Your Program):
┌─────────────────────────────┐
│  void* ptr;                 │
│  ┌──────────┐               │
│  │   ptr    │────→ 0x1080   │  ← ptr NOW POINTS to aligned memory!
│  └──────────┘               │
└─────────────────────────────┘
         │
         │ Points to
         ↓
HEAP (Allocated Memory):
┌──────────────────────────────────────────────────────────────┐
│                                                              │
│  0x1080                           0x10C0                     │
│  ↓                                ↓                          │
│  ┌────────────────────────────────┐                         │
│  │    Your 64 bytes (aligned!)    │                         │
│  │ ┌────┬────┬────┬────┬────┬────┐                         │
│  │ │f[0]│f[1]│f[2]│...│f[14]│f[15]│                         │
│  │ └────┴────┴────┴────┴────┴────┘                         │
│  └────────────────────────────────┘                         │
│  ↑                                ↑                          │
│  Start: 0x1080                    End: 0x10C0               │
│  (64-byte aligned!)               (Next boundary)           │
└──────────────────────────────────────────────────────────────┘
```

### **4. The & (Address-of) Operator is KEY!**

```cpp
posix_memalign(&ptr, ...)  // Pass ADDRESS of ptr, not ptr itself!
               ↑
               This is crucial!
```

**Why pass &ptr instead of ptr?**

```
WITHOUT & (WRONG - Won't compile):
───────────────────────────────────
posix_memalign(ptr, 64, 64)
               ↑
               Passes NULL (the value of ptr)
               Function can't modify ptr!

┌─────────────┐        ┌─────────────────┐
│ Your Code   │        │ posix_memalign  │
│ ptr = NULL  │───────→│ receives NULL   │
│             │        │ can't change ptr│
└─────────────┘        └─────────────────┘


WITH & (CORRECT):
─────────────────
posix_memalign(&ptr, 64, 64)
               ↑
               Passes the ADDRESS where ptr lives
               Function CAN modify ptr!

┌─────────────┐        ┌──────────────────────┐
│ Your Code   │        │ posix_memalign       │
│ ┌─────┐    │        │                      │
│ │ ptr │    │←───────│ Writes new address  │
│ └─────┘    │        │ directly to ptr!     │
│ at 0x7fff  │        │ *(&ptr) = 0x1080     │
└─────────────┘        └──────────────────────┘
```

### **5. Error Handling**

```cpp
if (posix_memalign(&ptr, alignment, size) != 0) {
    throw std::bad_alloc();
}
```

```
SUCCESS CASE (returns 0):
─────────────────────────
┌─────────────────┐
│ posix_memalign  │
│                 │
│ ✓ Found memory  │───→ Returns: 0
│ ✓ Aligned it    │     ptr = 0x1080 (valid address)
│ ✓ Updated ptr   │
└─────────────────┘

FAILURE CASE (returns non-zero):
─────────────────────────────────
┌─────────────────┐
│ posix_memalign  │
│                 │
│ ✗ Out of memory │───→ Returns: ENOMEM (12)
│   or            │     ptr = still nullptr!
│ ✗ Bad alignment │───→ Returns: EINVAL (22)
└─────────────────┘
                    ↓
            throw std::bad_alloc()
```

### **6. The static_cast<T*>(ptr)**

```cpp
return static_cast<T*>(ptr);
```

```
BEFORE CAST:                    AFTER CAST:
────────────                    ───────────
ptr is void*                    Returns T* (e.g., float*)
(Generic pointer)                (Typed pointer)

┌──────────┐                    ┌──────────────┐
│   void*  │                    │    float*    │
│  0x1080  │ ──────cast────→    │    0x1080    │
│          │                    │ Can do ptr[i]│
└──────────┘                    └──────────────┘
    ↑                                  ↑
Can't do arithmetic              Can do pointer arithmetic
Can't dereference                Can dereference as float
```

### **7. Complete Memory Layout Example**

```
Let's allocate 16 floats with 64-byte alignment:

CALL: aligned_alloc<float>(16, 64)

1. Initial:           ptr = nullptr → [ NULL ]

2. Call posix_memalign(&ptr, 64, 64):
   
   System Memory Search:
   ┌────────────────────────────────────────────────────┐
   │0x1000│0x1020│0x1040│0x1060│0x1080│0x10A0│0x10C0│   │
   │ Used │ Used │ Free │ Free │ Free │ Free │ Free │   │
   └────────────────────────────────────────────────────┘
           ↑       ↑       ↑       ↑       ↑       ↑
          Not    0x1040  Not    0x1080  Not    0x10C0
         Aligned  Good!  Aligned  Good! Aligned  Good!
   
   Picks 0x1040 (first aligned free block)

3. After allocation:  ptr → [ 0x1040 ]
   
   Memory at 0x1040:
   ┌─────────────────────────────────────────────────┐
   │Byte 0-3 │Byte 4-7 │Byte 8-11│...│Byte 60-63    │
   ├─────────┼─────────┼─────────┼───┼──────────────┤
   │ float[0]│ float[1]│ float[2]│...│ float[15]    │
   └─────────────────────────────────────────────────┘
   ↑                                                 ↑
   0x1040 (Aligned!)                                0x1080

4. Return static_cast<float*>(ptr):
   Returns: 0x1040 as float* type

5. Usage:
   float* data = aligned_alloc<float>(16, 64);
   __m512 vec = _mm512_load_ps(data);  // Aligned load - FAST!
```

### **Key Points Visualized**

```
posix_memalign MODIFIES ptr through its address:

     Your Variable              posix_memalign Function
    ┌────────────┐             ┌─────────────────────┐
    │    ptr     │←────────────│ Writes here using   │
    │  nullptr   │             │ the address &ptr    │
    └────────────┘             │                     │
     Address: 0x7fff1234       │ *(&ptr) = 0x1040    │
                               └─────────────────────┘
                                         ↓
    After call:                   Allocated Memory
    ┌────────────┐               ┌─────────────┐
    │    ptr     │──────────────→│  64 bytes   │
    │  0x1040    │               │  (aligned)  │
    └────────────┘               └─────────────┘
```

This is why `posix_memalign` needs the ADDRESS of your pointer (`&ptr`) - so it can modify what ptr points to!

Excellent question! Let me visualize how alignment works with 1000 floats and explain the relationship between data size and alignment boundaries:

## **Alignment vs Data Size - 1000 Floats Example**

### **1. Understanding the Difference**

```
ALIGNMENT ≠ TOTAL SIZE

Alignment = Starting address boundary (16, 32, or 64 bytes)
Size = Total memory needed (1000 floats × 4 = 4000 bytes)
```

### **2. Allocating 1000 Floats with 64-byte Alignment**

```cpp
float* data = aligned_alloc<float>(1000, 64);
                                    ↑     ↑
                          1000 floats    64-byte alignment
                          = 4000 bytes   (NOT the size!)
```

**What happens in memory:**

```
MEMORY ALLOCATION VISUALIZATION:

Request: 4000 bytes, aligned to 64-byte boundary

Step 1: Find address divisible by 64
────────────────────────────────────
Address     Hex        Divisible by 64?
0x103F      4159       4159 ÷ 64 = 64.98...  ✗
0x1040      4160       4160 ÷ 64 = 65.0      ✓ (65 × 64)
0x1041      4161       4161 ÷ 64 = 65.01...  ✗
...
0x1080      4224       4224 ÷ 64 = 66.0      ✓ (66 × 64)

Step 2: Allocate 4000 bytes starting at aligned address
────────────────────────────────────────────────────────

START ADDRESS: 0x1040 (aligned to 64)
END ADDRESS:   0x1040 + 4000 = 0x1FE0

┌─────────────────────────────────────────────────────────────┐
│                     Total: 4000 bytes                       │
├──────────────┬──────────────┬──────────────┬──────────────┤
│  First       │  Second      │     ...      │  Last        │
│  64 bytes    │  64 bytes    │              │  32 bytes    │
├──────────────┼──────────────┼──────────────┼──────────────┤
│ floats[0-15] │floats[16-31] │     ...      │floats[984-999]│
└──────────────┴──────────────┴──────────────┴──────────────┘
↑              ↑              ↑              ↑
0x1040         0x1080         0x10C0         0x1FE0
(aligned)      (aligned)      (aligned)      (end)

Cache lines:   [    1    ]    [    2    ]    [   62.5   ]
```

### **3. Why 64-byte Alignment for 4000 bytes?**

```
THE ALIGNMENT AFFECTS THE START ADDRESS, NOT THE TOTAL SIZE!

┌────────────────────────────────────────────────────────────┐
│                        4000 bytes total                    │
│  ┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬───┐ │
│  │ 64 │ 64 │ 64 │ 64 │ 64 │ 64 │ 64 │ 64 │ 64 │... │32 │ │
│  └────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴───┘ │
│  ↑                                                         │
│  Must start at address divisible by 64                    │
│  (0x1040, 0x1080, 0x10C0, etc.)                          │
└────────────────────────────────────────────────────────────┘

Benefits:
✓ First SIMD load is aligned
✓ All subsequent 64-byte chunks are aligned
✓ Cache line efficient
```

### **4. How SIMD Processes 1000 Floats**

```cpp
// Processing loop with AVX-512 (16 floats per iteration)
for (int i = 0; i < 1000; i += 16) {
    __m512 vec = _mm512_load_ps(&data[i]);
    // Process 16 floats at once
}
```

**Memory Access Pattern:**

```
Iteration 1: Load floats[0-15]   from 0x1040 (ALIGNED ✓)
Iteration 2: Load floats[16-31]  from 0x1080 (ALIGNED ✓)
Iteration 3: Load floats[32-47]  from 0x10C0 (ALIGNED ✓)
...
Iteration 62: Load floats[976-991] from 0x1FA0 (ALIGNED ✓)
Iteration 63: Load floats[992-999] from 0x1FE0 (PARTIAL)

Each 64-byte load:
┌────────────────────────────────────────────────────────┐
│            Single AVX-512 Load (64 bytes)              │
├──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┐     │
│f0│f1│f2│f3│f4│f5│f6│f7│f8│f9│fA│fB│fC│fD│fE│fF│     │
└──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┘     │
↑                                                 ↑      │
Aligned start                            Aligned end    │
└────────────────────────────────────────────────────────┘
```

### **5. Different Alignment Boundaries Compared**

```
SAME DATA (1000 floats = 4000 bytes), DIFFERENT ALIGNMENTS:

16-byte alignment (SSE):
─────────────────────────
Start: 0x1000, 0x1010, 0x1020, 0x1030... (any multiple of 16)
┌────┬────┬────┬────┬────────────────┐
│ 16 │ 16 │ 16 │ 16 │... (250 chunks)│
└────┴────┴────┴────┴────────────────┘
 SSE  SSE  SSE  SSE

32-byte alignment (AVX2):
──────────────────────────
Start: 0x1000, 0x1020, 0x1040, 0x1060... (any multiple of 32)
┌─────────┬─────────┬─────────────────┐
│   32    │   32    │... (125 chunks)│
└─────────┴─────────┴─────────────────┘
   AVX2      AVX2

64-byte alignment (AVX-512):
─────────────────────────────
Start: 0x1000, 0x1040, 0x1080, 0x10C0... (any multiple of 64)
┌──────────────┬──────────────┬───────┐
│      64      │      64      │...(63)│
└──────────────┴──────────────┴───────┘
    AVX-512        AVX-512
```

### **6. Memory Waste vs Performance Trade-off**

```
POTENTIAL "WASTE" WITH ALIGNMENT:

Scenario: System has free memory at 0x1033
          But we need 64-byte alignment

┌──────────────────────────────────────────────┐
│ Memory before allocation:                    │
│                                              │
│ 0x1000    0x1033    0x1040                  │
│    ↓         ↓         ↓                    │
│ [USED...][FREE>>>>>>>>>>>>>>>>]              │
│           ↑         ↑                        │
│           13 bytes  │                        │
│           "wasted"  Actual allocation starts │
└──────────────────────────────────────────────┘

Waste: 0x1040 - 0x1033 = 13 bytes unused
But: Gain 2-4x performance on SIMD operations!
```

### **7. Real Example: Processing Loop**

```cpp
// Allocate 1000 floats with 64-byte alignment
float* prices = aligned_alloc<float>(1000, 64);

// Fill with data
for (int i = 0; i < 1000; i++) {
    prices[i] = 100.0f + i * 0.01f;
}

// SIMD Processing with different instruction sets
#ifdef __AVX512F__
    // Process 16 at a time (64 bytes)
    for (int i = 0; i <= 1000 - 16; i += 16) {
        __m512 vec = _mm512_load_ps(&prices[i]);  // Aligned load
        // Each load starts at 64-byte boundary
        // i=0:   loads from 0x1040 (aligned)
        // i=16:  loads from 0x1080 (aligned)
        // i=32:  loads from 0x10C0 (aligned)
    }
#elif defined(__AVX2__)
    // Process 8 at a time (32 bytes)
    for (int i = 0; i <= 1000 - 8; i += 8) {
        __m256 vec = _mm256_load_ps(&prices[i]);  // Aligned load
        // i=0:   loads from 0x1040 (aligned to 64, thus also to 32)
        // i=8:   loads from 0x1060 (32-byte aligned)
        // i=16:  loads from 0x1080 (aligned)
    }
#endif
```

### **8. Alignment Math for 1000 Floats**

```
Total Size: 1000 × 4 = 4000 bytes

How many cache lines (64 bytes each)?
4000 ÷ 64 = 62.5 cache lines

Memory layout:
┌───────────────────────────────────────────────────┐
│ 62 complete cache lines + 32 bytes partial       │
├───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬────┤
│CL1│CL2│CL3│CL4│...│CL60│CL61│CL62│CL63│         │
│64B│64B│64B│64B│...│64B │64B │64B │32B │         │
└───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴────┘
↑                                           ↑
0x1040 (start, aligned)            0x1FE0 (end)

Each cache line holds: 64 ÷ 4 = 16 floats
Total: 62 × 16 + 8 = 992 + 8 = 1000 floats ✓
```

### **Key Takeaways**

1. **Alignment ≠ Size**: 64-byte alignment means START ADDRESS is divisible by 64, not that you're allocating only 64 bytes

2. **Any size works**: You can allocate 1 byte or 1GB with 64-byte alignment - it just affects WHERE it starts

3. **Performance benefit**: Even for 4000 bytes, starting at aligned address means:
   - First SIMD operation is fast
   - Most subsequent operations hit aligned addresses
   - Better cache utilization

4. **No reallocation needed**: The 64-byte alignment persists for the entire 4000-byte block - you don't need to realign every 64 bytes!

Let me break down these critical lines that form the core of the lead-lag detection algorithm:

## **Line-by-Line Explanation with Visualizations**

### **1. Sample Size Determination**
```cpp
int n_samples = std::min(prices1.size(), prices2.size());
```

**Purpose:** Ensures we don't go out of bounds when comparing two series

```
Example:
prices1: [100.1, 100.2, 100.3, 100.4, 100.5]  size = 5
prices2: [99.9, 100.0, 100.1]                  size = 3
         ↓
n_samples = min(5, 3) = 3  ← Only compare first 3 elements

Why? Correlation needs pairs of values:
(prices1[0], prices2[0]) ✓
(prices1[1], prices2[1]) ✓
(prices1[2], prices2[2]) ✓
(prices1[3], prices2[3]) ✗ prices2[3] doesn't exist!
```

### **2. Number of Lags Calculation**
```cpp
int num_lags = (2 * config.max_lag_ns / config.lag_step_ns) + 1;
```

**Purpose:** Calculates how many different lag values to test

```
Example: max_lag_ns = 1000, lag_step_ns = 100

Range: -1000ns to +1000ns in steps of 100ns

-1000  -900  -800  -700  -600  -500  -400  -300  -200  -100  0  +100  +200  +300  +400  +500  +600  +700  +800  +900  +1000
  ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓   ↓   ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓     ↓
 [0]   [1]   [2]   [3]   [4]   [5]   [6]   [7]   [8]   [9] [10] [11]  [12]  [13]  [14]  [15]  [16]  [17]  [18]  [19]  [20]

Total range: 2 * 1000 = 2000ns
Number of steps: 2000 / 100 = 20
Plus center (0): 20 + 1 = 21 lags

Formula: (2 * max_lag_ns / lag_step_ns) + 1 = (2 * 1000 / 100) + 1 = 21
```

### **3. Correlations Vector**
```cpp
std::vector<float> correlations(num_lags);
```

**Purpose:** Pre-allocates storage for correlation at each lag

```
correlations vector (21 elements for our example):
┌────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┬────┐
│[0] │[1] │[2] │[3] │[4] │[5] │[6] │[7] │[8] │[9] │[10]│[11]│[12]│[13]│[14]│[15]│[16]│[17]│[18]│[19]│[20]│
└────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┴────┘
  ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓    ↓
-1000 -900 -800 -700 -600 -500 -400 -300 -200 -100   0  +100 +200 +300 +400 +500 +600 +700 +800 +900 +1000
                                                      ↑
                                                 No lag (baseline)
```

### **4. OpenMP Parallel Directive**
```cpp
#pragma omp parallel for schedule(dynamic)
```

**This is OpenMP (Open Multi-Processing) - breaks the loop across CPU cores:**

```
#pragma omp parallel for schedule(dynamic)
   ↑       ↑         ↑       ↑        ↑
   │       │         │       │        └── Dynamic scheduling
   │       │         │       └────────── For loop parallelization
   │       │         └────────────────── Creates thread team
   │       └──────────────────────────── OpenMP library
   └──────────────────────────────────── Compiler directive

What it does:
┌─────────────────────────────────────────────────────┐
│            Single Thread (WITHOUT OpenMP)           │
├─────────────────────────────────────────────────────┤
│ Core 0: lag[0]→lag[1]→lag[2]→...→lag[20]           │
│ Time: ████████████████████████████████████ (21 units)│
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│           Multi-Thread (WITH OpenMP - 4 cores)      │
├─────────────────────────────────────────────────────┤
│ Core 0: lag[0]  lag[4]  lag[8]  lag[12] lag[16]    │
│ Core 1: lag[1]  lag[5]  lag[9]  lag[13] lag[17]    │
│ Core 2: lag[2]  lag[6]  lag[10] lag[14] lag[18]    │
│ Core 3: lag[3]  lag[7]  lag[11] lag[15] lag[19]    │
│ Time: ██████ (6 units) - 3.5x faster!               │
└─────────────────────────────────────────────────────┘

schedule(dynamic) means:
- static:  Each thread gets fixed chunk (0-5, 6-11, 12-17, 18-20)
- dynamic: Threads grab next available task when ready (better load balancing)
```

### **5. The Parallel Loop**
```cpp
for (int lag_idx = 0; lag_idx < num_lags; lag_idx++) {
    int lag_ns = -config.max_lag_ns + lag_idx * config.lag_step_ns;
    int lag_samples = lag_ns / 100;
```

**Converting lag index to actual lag values:**

```
lag_idx = 0:   lag_ns = -1000 + 0*100 = -1000ns   lag_samples = -1000/100 = -10
lag_idx = 1:   lag_ns = -1000 + 1*100 = -900ns    lag_samples = -900/100 = -9
lag_idx = 2:   lag_ns = -1000 + 2*100 = -800ns    lag_samples = -800/100 = -8
...
lag_idx = 10:  lag_ns = -1000 + 10*100 = 0ns      lag_samples = 0/100 = 0
...
lag_idx = 20:  lag_ns = -1000 + 20*100 = +1000ns  lag_samples = 1000/100 = +10

Visual of what each lag means:
Lag = -10 samples (Series 2 leads by 10):
Series1: [0][1][2][3][4][5][6][7][8][9][10][11][12]...
Series2:           [0][1][2][3][4][5][6][7][8][9]...
                    ↑ Aligned with ↑

Lag = +10 samples (Series 1 leads by 10):
Series1:           [0][1][2][3][4][5][6][7][8][9]...
Series2: [0][1][2][3][4][5][6][7][8][9][10][11][12]...
                    ↑ Aligned with ↑
```

### **6. Computing and Storing Correlations**
```cpp
float correlation = computeCorrelationAtLagSIMD(prices1.data(), prices2.data(), 
                                               n_samples, lag_samples);
correlations[lag_idx] = correlation;
```

**Each thread computes correlation for its assigned lag:**

```
Thread Assignment (dynamic scheduling):
Thread 0 computes lag_idx=0  → correlations[0] = 0.45
Thread 1 computes lag_idx=1  → correlations[1] = 0.52
Thread 2 computes lag_idx=2  → correlations[2] = 0.61
Thread 3 computes lag_idx=3  → correlations[3] = 0.73
Thread 0 finishes, grabs lag_idx=4 → correlations[4] = 0.85
Thread 1 finishes, grabs lag_idx=5 → correlations[5] = 0.92
...

Result array after parallel execution:
correlations = [0.45, 0.52, 0.61, 0.73, 0.85, 0.92, 0.97, 0.99, 0.98, 0.95, 0.88, ...]
```

### **7. Finding Maximum Correlation**
```cpp
auto max_it = std::max_element(correlations.begin(), correlations.end());
int max_idx = std::distance(correlations.begin(), max_it);
int optimal_lag_ns = -config.max_lag_ns + max_idx * config.lag_step_ns;
```

**Finding which lag has the highest correlation:**

```
Step 1: Find maximum element
correlations = [0.45, 0.52, 0.61, 0.73, 0.85, 0.92, 0.97, 0.99, 0.98, 0.95, 0.88, ...]
                                                            ↑
                                                        max_it points here
                                                        Value: 0.99
Step 2: Get index
max_idx = distance(begin, max_it) = 7

Step 3: Convert index back to nanoseconds
optimal_lag_ns = -1000 + 7*100 = -1000 + 700 = -300ns

Interpretation:
Maximum correlation of 0.99 occurs at lag = -300ns
This means Series 2 leads Series 1 by 300 nanoseconds!
```

### **Complete Flow Visualization**

```
Input: Two price series, max_lag=1000ns, step=100ns
                    ↓
      Calculate 21 different lags to test
                    ↓
    ┌───────────────────────────────────┐
    │     OpenMP: Split across cores     │
    ├────────┬────────┬────────┬────────┤
    │ Core 0 │ Core 1 │ Core 2 │ Core 3 │
    │  SIMD  │  SIMD  │  SIMD  │  SIMD  │
    └────────┴────────┴────────┴────────┘
                    ↓
    Correlations: [0.45, 0.52, ..., 0.99, ..., 0.88]
                    ↓
            Find maximum: 0.99 at index 7
                    ↓
            Optimal lag: -300 nanoseconds
                    ↓
    Result: Series 2 leads Series 1 by 300ns
```

The beauty of this approach:
1. **Parallel**: Multiple cores test different lags simultaneously
2. **SIMD**: Each core uses vector instructions for fast correlation
3. **Comprehensive**: Tests all possible lags in the range
4. **Precise**: Finds the exact lag with maximum correlation
